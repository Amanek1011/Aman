import os
from datetime import datetime, timedelta

from aiogram import Router, types

import aiohttp
import xml.etree.ElementTree as ET

from bs4 import BeautifulSoup
from dotenv import load_dotenv
import keyboards as kb
load_dotenv()
WEATHER_API_KEY = os.getenv('WEATHER_API_KEY')

services_router = Router(name="services_router")


async def news_handler(message: types.Message):
    await message.answer('Ğ—Ğ° ĞºĞ°ĞºĞ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ°Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸?', reply_markup=kb.inline_news)



async def weather_handler(message: types.Message):
    weather = await get_weather()
    await message.answer(weather)



async def currency_handler(message: types.Message):
    rates = await get_currency_rates()
    await message.answer(rates)



async def back_to_main(message: types.Message):
    await message.answer(
        "Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ÑÑ Ğ² Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¼ĞµĞ½Ñ",
        reply_markup=kb.reply_menu
    )


async def get_currency_rates():
    url = "https://www.nbkr.kg/XML/daily.xml"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                xml_data = await response.text()
                root = ET.fromstring(xml_data)
                rates = {}
                for currency in root.findall('Currency'):
                    iso_code = currency.get('ISOCode')
                    value_element = currency.find('Value')
                    if iso_code and value_element is not None:
                        rates[iso_code] = value_element.text

                return (
                    "ğŸ’° <b>ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºÑƒÑ€ÑÑ‹ Ğ²Ğ°Ğ»ÑÑ‚:</b>\n\n"
                    f"ğŸ‡ºğŸ‡¸ USD: {rates.get('USD', 'N/A')} KGS\n"
                    f"ğŸ‡ªğŸ‡º EUR: {rates.get('EUR', 'N/A')} KGS\n"
                    f"ğŸ‡·ğŸ‡º RUB: {rates.get('RUB', 'N/A')} KGS\n"
                    f"ğŸ‡°ğŸ‡¿ KZT: {rates.get('KZT', 'N/A')} KGS\n"
                    f"ğŸ‡¨ğŸ‡³ CNY: {rates.get('CNY', 'N/A')} KGS"
                )
            return "âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºÑƒÑ€ÑÑ‹ Ğ²Ğ°Ğ»ÑÑ‚"


async def get_weather():
    url = f"https://api.openweathermap.org/data/2.5/weather?q=Ğ‘Ğ¸ÑˆĞºĞµĞº&appid={WEATHER_API_KEY}&units=metric&lang=ru"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.json()
                weather = data["weather"][0]["description"].capitalize()
                temp = data["main"]["temp"]
                feels = data["main"]["feels_like"]
                humidity = data["main"]["humidity"]
                return (
                    f"ğŸŒ¤ <b>ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ° Ğ² Ğ‘Ğ¸ÑˆĞºĞµĞºĞµ:</b>\n\n"
                    f"â€¢ Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ: {weather}\n"
                    f"â€¢ Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°: {temp}Â°C\n"
                    f"â€¢ ĞÑ‰ÑƒÑ‰Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº: {feels}Â°C\n"
                    f"â€¢ Ğ’Ğ»Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ: {humidity}%"
                )
            return "âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğµ"

async def get_news_today():
    current_data = datetime.now().strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={current_data}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    sections = soup.find_all('div', class_="Tag--articles")

                    if not sections:
                        print("No articles found with class 'Tag--articles'.")
                        return "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸."

                    news_list = []

                    for tag in sections:
                        tags = tag.find_all('div', class_="Tag--article")
                        if not tags:
                            print("No articles found in the section.")
                            return "Ğ’ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."

                        for i in tags:
                            try:
                                site = i.find('a', class_='ArticleItem--name')['href']
                                title = i.find('a', class_='ArticleItem--name').text.strip()
                                time = i.find('div', class_='ArticleItem--time').text.strip()
                                news_list.append(f'ğŸ•‘ {time} | {title}'
                                                 f'  {site}')
                            except Exception as e:
                                print(f"Error extracting article data: {e}")

                    if not news_list:
                        print("No news articles were successfully parsed.")
                        return "ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹."

                    return "\n\n".join(news_list)

                else:
                    print(f"Failed to retrieve the page. Status code: {response.status}")
                    return "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹."

    except Exception as e:
        print(f"Error during request or parsing: {e}")
        return "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."



async def get_data_today():
    current_data = datetime.now().strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={current_data}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    data = soup.find('span',class_='PaginatorDate--today-text').text
                    return f' {data.replace('  ','')}'
                else:
                    print('ĞĞµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñƒ')
    except Exception as e:
        print(e)


async def get_news_yesterday():
    current_data = datetime.now()
    yesterday = current_data - timedelta(days=1)
    formatted_yesterday = yesterday.strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={formatted_yesterday}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    sections = soup.find_all('div', class_="Tag--articles")

                    if not sections:
                        print("No articles found with class 'Tag--articles'.")
                        return "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸."

                    news_list = []

                    for tag in sections:
                        tags = tag.find_all('div', class_="Tag--article")
                        if not tags:
                            print("No articles found in the section.")
                            return "Ğ’ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."

                        for i in tags:
                            try:
                                site = i.find('a', class_='ArticleItem--name')['href']
                                title = i.find('a', class_='ArticleItem--name').text.strip()
                                time = i.find('div', class_='ArticleItem--time').text.strip()
                                news_list.append(f'ğŸ•‘ {time} | {title}'
                                                 f'  {site}')
                            except Exception as e:
                                print(f"Error extracting article data: {e}")

                    if not news_list:
                        print("No news articles were successfully parsed.")
                        return "ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹."

                    return "\n\n".join(news_list)

                else:
                    print(f"Failed to retrieve the page. Status code: {response.status}")
                    return "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹."

    except Exception as e:
        print(f"Error during request or parsing: {e}")
        return "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."

async def get_data_yesterday():
    current_data = datetime.now()
    yesterday = current_data - timedelta(days=1)
    formatted_yesterday = yesterday.strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={formatted_yesterday}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    data = soup.find('span',class_='PaginatorDate--today-text').text
                    return f' {data.replace('  ','')}'
                else:
                    print('ĞĞµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñƒ')
    except Exception as e:
        print(e)

async def send_long_message(message, text):
    max_length = 4096
    while len(text) > max_length:
        await message.answer(text[:max_length], disable_web_page_preview=True)
        text = text[max_length:]
    if text:
        await message.answer(text, disable_web_page_preview=True)