import os
from datetime import datetime, timedelta

from aiogram import Router, types

import aiohttp
import xml.etree.ElementTree as ET

from bs4 import BeautifulSoup
from dotenv import load_dotenv
import keyboards as kb
load_dotenv()
WEATHER_API_KEY = os.getenv('WEATHER_API_KEY')

services_router = Router(name="services_router")


async def news_handler(message: types.Message):
    await message.answer('Ð—Ð° ÐºÐ°ÐºÐ¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð²Ð°Ð¼ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸?', reply_markup=kb.inline_news)



async def weather_handler(message: types.Message):
    weather = await get_weather()
    await message.answer(weather)



async def currency_handler(message: types.Message):
    rates = await get_currency_rates()
    await message.answer(rates)



async def back_to_main(message: types.Message):
    await message.answer(
        "Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ÑÑ Ð² Ð³Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ",
        reply_markup=kb.reply_menu
    )


async def get_currency_rates():
    url = "https://www.nbkr.kg/XML/daily.xml"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                xml_data = await response.text()
                root = ET.fromstring(xml_data)
                rates = {}
                for currency in root.findall('Currency'):
                    iso_code = currency.get('ISOCode')
                    value_element = currency.find('Value')
                    if iso_code and value_element is not None:
                        rates[iso_code] = value_element.text

                return (
                    "ðŸ’° ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÑƒÑ€ÑÑ‹ Ð²Ð°Ð»ÑŽÑ‚: \n\n"
                    f"ðŸ‡ºðŸ‡¸ USD: {rates.get('USD', 'N/A')} KGS\n"
                    f"ðŸ‡ªðŸ‡º EUR: {rates.get('EUR', 'N/A')} KGS\n"
                    f"ðŸ‡·ðŸ‡º RUB: {rates.get('RUB', 'N/A')} KGS\n"
                    f"ðŸ‡°ðŸ‡¿ KZT: {rates.get('KZT', 'N/A')} KGS\n"
                    f"ðŸ‡¨ðŸ‡³ CNY: {rates.get('CNY', 'N/A')} KGS"
                )
            return "âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÑƒÑ€ÑÑ‹ Ð²Ð°Ð»ÑŽÑ‚"


async def get_weather():
    url = f"https://api.openweathermap.org/data/2.5/weather?q=Ð‘Ð¸ÑˆÐºÐµÐº&appid={WEATHER_API_KEY}&units=metric&lang=ru"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.json()
                weather = data["weather"][0]["description"].capitalize()
                temp = data["main"]["temp"]
                feels = data["main"]["feels_like"]
                humidity = data["main"]["humidity"]
                return (
                    f"ðŸŒ¤ ÐŸÐ¾Ð³Ð¾Ð´Ð° Ð² Ð‘Ð¸ÑˆÐºÐµÐºÐµ: \n\n"
                    f"â€¢ Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ: {weather}\n"
                    f"â€¢ Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°: {temp}Â°C\n"
                    f"â€¢ ÐžÑ‰ÑƒÑ‰Ð°ÐµÑ‚ÑÑ ÐºÐ°Ðº: {feels}Â°C\n"
                    f"â€¢ Ð’Ð»Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ: {humidity}%"
                )
            return "âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ð¿Ð¾Ð³Ð¾Ð´Ðµ"

async def get_news_today():
    current_data = datetime.now().strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={current_data}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    sections = soup.find_all('div', class_="Tag--articles")

                    if not sections:
                        print("No articles found with class 'Tag--articles'.")
                        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸."

                    news_list = []

                    for tag in sections:
                        tags = tag.find_all('div', class_="Tag--article")
                        if not tags:
                            print("No articles found in the section.")
                            return "Ð’ Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚ÑŒÐ¸."

                        for i in tags:
                            try:
                                site = i.find('a', class_='ArticleItem--name')['href']
                                title = i.find('a', class_='ArticleItem--name').text.strip()
                                time = i.find('div', class_='ArticleItem--time').text.strip()
                                news_list.append(f'ðŸ•‘ {time} | {title}'
                                                 f'  {site}')
                            except Exception as e:
                                print(f"Error extracting article data: {e}")

                    if not news_list:
                        print("No news articles were successfully parsed.")
                        return "ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹."

                    return "\n\n".join(news_list)

                else:
                    print(f"Failed to retrieve the page. Status code: {response.status}")
                    return "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹."

    except Exception as e:
        print(f"Error during request or parsing: {e}")
        return "ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ñ…."



async def get_data_today():
    current_data = datetime.now().strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={current_data}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    data = soup.find('span',class_='PaginatorDate--today-text').text
                    return f' {data.replace('  ','')}'
                else:
                    print('ÐÐµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ñƒ')
    except Exception as e:
        print(e)


async def get_news_yesterday():
    current_data = datetime.now()
    yesterday = current_data - timedelta(days=1)
    formatted_yesterday = yesterday.strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={formatted_yesterday}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    sections = soup.find_all('div', class_="Tag--articles")

                    if not sections:
                        print("No articles found with class 'Tag--articles'.")
                        return "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸."

                    news_list = []

                    for tag in sections:
                        tags = tag.find_all('div', class_="Tag--article")
                        if not tags:
                            print("No articles found in the section.")
                            return "Ð’ Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚ÑŒÐ¸."

                        for i in tags:
                            try:
                                site = i.find('a', class_='ArticleItem--name')['href']
                                title = i.find('a', class_='ArticleItem--name').text.strip()
                                time = i.find('div', class_='ArticleItem--time').text.strip()
                                news_list.append(f'ðŸ•‘ {time} | {title}'
                                                 f'  {site}')
                            except Exception as e:
                                print(f"Error extracting article data: {e}")

                    if not news_list:
                        print("No news articles were successfully parsed.")
                        return "ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹."

                    return "\n\n".join(news_list)

                else:
                    print(f"Failed to retrieve the page. Status code: {response.status}")
                    return "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹."

    except Exception as e:
        print(f"Error during request or parsing: {e}")
        return "ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ñ…."

async def get_data_yesterday():
    current_data = datetime.now()
    yesterday = current_data - timedelta(days=1)
    formatted_yesterday = yesterday.strftime("%Y-%m-%d")
    url = f'https://kaktus.media/?lable=8&date={formatted_yesterday}&order=time'
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'lxml')
                    data = soup.find('span',class_='PaginatorDate--today-text').text
                    return f' {data.replace('  ','')}'
                else:
                    print('ÐÐµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ñƒ')
    except Exception as e:
        print(e)

async def send_long_message(message, text):
    max_length = 4096
    while len(text) > max_length:
        await message.answer(text[:max_length], disable_web_page_preview=True)
        text = text[max_length:]
    if text:
        await message.answer(text, disable_web_page_preview=True)